{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yajuna/navier-stokes/blob/master/navier_stokes_tf2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook implements the physics informed neural network methodology proposed by [Raissi et al](https://github.com/maziarraissi/PINNs), and is a Tensorflow 2 modification based on [Jan Blechschmidt and Oliver G. Ernst](https://github.com/janblechschmidt/PDEsByNNs/blob/main/PINN_Solver.ipynb).\n",
        "\n",
        "In this notebook, we construct a physics informed neural network to solve the Navier-Stokes equation, and validate the methodology via\n",
        "1. use the original data from Raissi's original work\n",
        "2. use our own simulated data generated from OpenFoam\n",
        "\n",
        "Finally, we apply the PINN methodology to measured data to compute the solution to Navier-Stokes equation."
      ],
      "metadata": {
        "id": "dsdTAFcmWIsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import libraries. We mainly use Python's `Numpy` library, and the `Tensorflow` library. Visualization is performed by `Matplotlib` library."
      ],
      "metadata": {
        "id": "4qS_CUXuYaeD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qJZlBC2TKCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e21baf50-09cd-428d-8b70-e1971840a090"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3 version is 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n",
            "Numpy version is 1.25.2\n",
            "Tensorflow version is 2.15.0\n",
            "Matplotlib version is 3.7.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas\n",
        "import scipy.io\n",
        "\n",
        "from time import time\n",
        "\n",
        "tf.experimental.numpy.experimental_enable_numpy_behavior()\n",
        "\n",
        "import sys\n",
        "print(\"Python 3 version is\", sys.version)\n",
        "print(\"Numpy version is\", np.__version__)\n",
        "print(\"Tensorflow version is\", tf.__version__)\n",
        "import matplotlib\n",
        "print(\"Matplotlib version is\", matplotlib.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = np.array([[5,3],[3,2]])\n",
        "b = np.array([[1],[1]])\n",
        "sol = np.linalg.solve(A,b)\n",
        "print(sol)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5LmOJzxBiU8",
        "outputId": "80812685-8fcb-4526-cba0-781900a58b1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.]\n",
            " [ 2.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone Raissi's repository and import data for training the neural network."
      ],
      "metadata": {
        "id": "k5If-ZYYZYSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/maziarraissi/PINNs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOFcAXh1TUCS",
        "outputId": "4f30ba22-0be0-4e36-b6fb-445c4901327a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'PINNs' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define path for Raissi's repository."
      ],
      "metadata": {
        "id": "D_0Wyrn2Zp3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# \".\" for Colab/VSCode, and \"..\" for GitHub\n",
        "repoPath = os.path.join(\".\", \"PINNs\")\n",
        "# repoPath = os.path.join(\"..\", \"PINNs\")\n",
        "utilsPath = os.path.join(repoPath, \"Utilities\")\n",
        "dataPath = os.path.join(repoPath, \"main\", \"Data\")\n",
        "appDataPath = os.path.join(repoPath, \"appendix\", \"Data\")"
      ],
      "metadata": {
        "id": "bi8ZKXvqTWjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define data type to be `float64`; set random seed for reproducibility."
      ],
      "metadata": {
        "id": "7VlAgJhHajQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set data type\n",
        "DTYPE='float64'\n",
        "tf.keras.backend.set_floatx(DTYPE)\n",
        "\n",
        "# Set random seed for reproducible results\n",
        "tf.random.set_seed(0)"
      ],
      "metadata": {
        "id": "B_gYHSTFTZHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import data file `cylinder_nektar_wake.mat`. This set of high resolution data was generated by the spectral/hp-element solver NekTar. Implementation details is described [here](https://www.sciencedirect.com/science/article/abs/pii/S0021999118307125).\n",
        "\n",
        "The data corresponds to the solution of the Navier-Stokes equation with the following boundary conditions of the [1, 8] by [-2,2] domain:\n",
        "\n",
        "1. left boundary: uniform free stream velocity\n",
        "2. right boundary: zero pressure outflow, located downstream of the cylinder\n",
        "3. top and bottom: periodic condition\n",
        "\n",
        "The notations for the dataset are explained below:\n",
        "\n",
        "`N`: number of grid points in space\n",
        "`T`: number of time steps\n",
        "\n",
        "`U_star`: $x$- component of the velocity field $u$ and $y$- component of the velocity field $v$ values, for all U and T values. Read to `UU` and `VV`, and later reshaped to `u` and `v`. Each of size NT.\n",
        "\n",
        "`P_star`: pressure $p$ for all N and T. Read to `PP`, then reshaped to `p`. Each of size NT.\n",
        "\n",
        "`t`: time where data are recorded, of size T. Broadcasted to size NT.\n",
        "\n",
        "`X_star`: spatial variables where data are recorded, including both $x$ and $y$. Read to `XX` and `YY`, then reshaped to `x` and `y`. Each broadcasted to size NT.\n",
        "\n",
        "Training data was labeled `*_train`, with spatial, temporal, as well as velocity and pressure data. The training data was chosen randomly from the full dataset explained above, of size `N_train`.\n",
        "\n",
        "Testing data was labeled `*_star`, chosen at a time step `snap`.\n",
        "\n",
        "Input of the neural network is `X_train` and `data_train`, with $x$, $y$, and $t$ variables, as well as $u$, $v$, and $p$. Testing data is similar.\n",
        "\n",
        "Training data consist of 5000 data points, chosen from NT points, denoted by `N_train`. Testing data was chosen at a random time snap, denoted by `snap`."
      ],
      "metadata": {
        "id": "4_ktA5V_aq_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_train = 5000\n",
        "\n",
        "path = os.path.join(dataPath, \"cylinder_nektar_wake.mat\")\n",
        "data = scipy.io.loadmat(path)\n",
        "\n",
        "# define training and testing data\n",
        "U_star = data['U_star'] # N x 2 x T\n",
        "P_star = data['p_star'] # N x T\n",
        "t = data['t'] # T x 1\n",
        "X_star = data['X_star'] # N x 2\n",
        "\n",
        "N = X_star.shape[0]\n",
        "T = t.shape[0]\n",
        "\n",
        "print('Size of x: N and size of t: T', N, T)\n",
        "\n",
        "#tile independent variable Data (like meshgrid) to size N by T\n",
        "XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
        "YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
        "TT = np.tile(t, (1,N)).T # N x T\n",
        "\n",
        "UU = U_star[:,0,:] # N x T\n",
        "VV = U_star[:,1,:] # N x T\n",
        "PP = P_star # N x T\n",
        "\n",
        "x = XX.flatten()[:,None] # NT x 1\n",
        "y = YY.flatten()[:,None] # NT x 1\n",
        "t = TT.flatten()[:,None] # NT x 1\n",
        "\n",
        "u = UU.flatten()[:,None] # NT x 1\n",
        "v = VV.flatten()[:,None] # NT x 1\n",
        "p = PP.flatten()[:,None] # NT x 1\n",
        "\n",
        "######################################################################\n",
        "######################## Noiseless Data ##############################\n",
        "######################################################################\n",
        "# Training Data\n",
        "idx = np.random.choice(N*T, N_train, replace=False)\n",
        "x_train = x[idx,:]\n",
        "y_train = y[idx,:]\n",
        "t_train = t[idx,:]\n",
        "\n",
        "u_train = u[idx,:]\n",
        "v_train = v[idx,:]\n",
        "p_train = p[idx,:]\n",
        "\n",
        "# Test Data -- used in model.predict to test model\n",
        "snap = np.array([100])\n",
        "x_star = X_star[:,0:1]\n",
        "y_star = X_star[:,1:2]\n",
        "t_star = TT[:,snap]\n",
        "\n",
        "u_star = U_star[:,0,snap]\n",
        "v_star = U_star[:,1,snap]\n",
        "p_star = P_star[:,snap]\n",
        "\n",
        "# make training and testing data\n",
        "X_train = tf.concat([x_train,y_train,t_train], axis = 1)\n",
        "data_train = tf.concat([u_train,v_train,p_train], axis = 1) ###First occurrence of data_train\n",
        "\n",
        "X_test = tf.concat([x_star,y_star,t_star], axis = 1)\n",
        "data_test = tf.concat([u_star,v_star,p_star], axis = 1)\n",
        "\n",
        "print(np.min(YY), np.max(YY), np.min(XX), np.max(XX))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHgN4u9mTge_",
        "outputId": "e63028c7-5fb5-4223-9f53-413a973fcffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of x: N and size of t: T 5000 200\n",
            "-2.0 2.0 1.0 8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### We define the residual of the PDE. The physics information is given by the governing Navier-Stokes equations:\n",
        "\n",
        "$u_t+(uu_x + vu_y)=-p_x+0.01(u_{xx}+u_{yy})$\n",
        "\n",
        "$v_t+(uv_x + vv_y)=-p_y+0.01(v_{xx}+v_{yy})$,\n",
        "\n",
        "here $(x,y)$ denotes the spatial variable, $t$ the temporal variable, $u(t,x,y)$ denotes the $x$-component of the velocity field, $v(t,x,y)$ the y-component, and $p(t,x,y)$ the pressure.\n",
        "\n",
        "We include the following errors in the loss term of the neural network\n",
        "\n",
        "$f = u_t+(uu_x + vu_y)+p_x-0.01(u_{xx}+u_{yy})$\n",
        "\n",
        "$g = v_t+(uv_x + vv_y)+p_y-0.01(v_{xx}+v_{yy})$,\n",
        "\n",
        "and we train the neural network by minimizing the loss including the above terms.\n",
        "\n"
      ],
      "metadata": {
        "id": "wzZjh3Hh93tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define residual of the PDE\n",
        "def fun_r(u, u_t, u_x, u_y, u_xx, u_yy, v, v_t, v_x, v_y, v_xx, v_yy, p_x, p_y):\n",
        "  f = u_t + (u * u_x + v * u_y) + p_x - 0.01 * (u_xx + u_yy)\n",
        "  g = v_t + (u * v_x + v * v_y) + p_y - 0.01 * (v_xx + v_yy)\n",
        "  print('tf.concat([f,g], axis = 1).shape', tf.concat([f,g], axis = 1).shape)\n",
        "  return tf.concat([f,g], axis = 1)"
      ],
      "metadata": {
        "id": "-ad4iGrWmJnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the neural network; by default, the neural network has 8 hidden layers, and 20 neurons on each layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "3iajUNDXUP_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_model(num_hidden_layers=8, num_neurons_per_layer=20):\n",
        "    # Initialize a feedforward neural network\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    # Input is three-dimensional (time + two spatial dimensions)\n",
        "    model.add(tf.keras.Input(3))\n",
        "\n",
        "    # # Define lower boundary and upper boundary\n",
        "    # x = X_train[:,0:1]\n",
        "    # y = X_train[:,1:2]\n",
        "    # t = X_train[:,2:3]\n",
        "    # lb = tf.constant([np.array(x.min()), np.array(y.min()), np.array(t.min())])\n",
        "    # ub = tf.constant([np.array(x.max()), np.array(y.max()), np.array(t.max())])\n",
        "\n",
        "    # Append hidden layers\n",
        "    for _ in range(num_hidden_layers):\n",
        "        model.add(tf.keras.layers.Dense(num_neurons_per_layer,\n",
        "            activation=tf.keras.activations.get('tanh'),\n",
        "            kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Output is two-dimensional: psi and p\n",
        "    model.add(tf.keras.layers.Dense(2))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "0AGQveCbUPFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_r(model, X_train):\n",
        "\n",
        "    # A tf.GradientTape is used to compute derivatives in TensorFlow\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # Split t and x to compute partial derivatives\n",
        "        x, y, t = X_train[:, 0:1], X_train[:,1:2], X_train[:,2:3]\n",
        "\n",
        "        # Variables x,y,t are watched during tape\n",
        "        # to compute derivatives\n",
        "        tape.watch(x)\n",
        "        tape.watch(y)\n",
        "        tape.watch(t)\n",
        "\n",
        "        # Determine residual\n",
        "        pp = model(tf.stack([x[:,0],y[:,0],t[:,0]], axis=1))\n",
        "        psi = pp[:,0:1]\n",
        "        p = pp[:,1:2]\n",
        "        # Compute gradients within the GradientTape\n",
        "        # for all needed derivatives\n",
        "        u = tape.gradient(psi, y)\n",
        "        print(\"max and min of u\", u.max, u.min)\n",
        "        v = -1 * tape.gradient(psi, x)\n",
        "        print(\"type of v\", type(v))\n",
        "        u_x = tape.gradient(u, x)\n",
        "        print(\"max and min of u_x\", u_x.max, u_x.min)\n",
        "        u_y = tape.gradient(u, y)\n",
        "        print(\"max and min of u_y\", u_y.max, u_y.min)\n",
        "        v_x = tape.gradient(v, x)\n",
        "        print(\"max and min v_x\", v_x.max, v_x.min)\n",
        "        v_y = tape.gradient(v, y)\n",
        "\n",
        "\n",
        "\n",
        "    p_x = tape.gradient(p, x)\n",
        "    p_y = tape.gradient(p, y)\n",
        "\n",
        "    u_t = tape.gradient(u, t)\n",
        "    u_xx = tape.gradient(u_x, x)\n",
        "    u_yy = tape.gradient(u_y, y)\n",
        "\n",
        "    v_t = tape.gradient(v, t)\n",
        "    v_xx = tape.gradient(v_x, x)\n",
        "    v_yy = tape.gradient(v_y, y)\n",
        "\n",
        "    del tape\n",
        "\n",
        "    return fun_r(u, u_t, u_x, u_y, u_xx, u_yy, v, v_t, v_x, v_y, v_xx, v_yy, p_x, p_y)"
      ],
      "metadata": {
        "id": "kVuZPS6uUf94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### This is attempting to fix the shape error ##########\n",
        "\n",
        "def compute_loss(model, X_train, data_train):\n",
        "\n",
        "    # Compute phi^r\n",
        "    r = get_r(model, X_train)\n",
        "    phi_r = tf.reduce_mean(tf.square(r))\n",
        "\n",
        "    # Initialize loss\n",
        "    loss = phi_r\n",
        "    # u_pred = model(X_train)\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # Split t and x to compute partial derivatives\n",
        "        x, y, t = X_train[:, 0:1], X_train[:,1:2], X_train[:,2:3]\n",
        "\n",
        "        # Variables x,y,t are watched during tape\n",
        "        # to compute derivatives\n",
        "        tape.watch(x)\n",
        "        tape.watch(y)\n",
        "        tape.watch(t)\n",
        "\n",
        "        # Determine residual\n",
        "        pp = model(tf.stack([x[:,0],y[:,0],t[:,0]], axis=1))\n",
        "        psi = pp[:,0:1]\n",
        "        p = pp[:,1:2]\n",
        "\n",
        "        u = tape.gradient(psi, y)\n",
        "        v = -1 * tape.gradient(psi, x)\n",
        "        del tape\n",
        "\n",
        "        # Concatenate u, v, and p along with p to match the shape of data_train\n",
        "        data_pred = tf.concat([u, v, p], axis=1)\n",
        "\n",
        "    ################Added by Yoshi##################\n",
        "    print(\"\")\n",
        "    print(\"################Added by Yoshi##################\")\n",
        "    print(\"This is data_pred.shape\")\n",
        "    print(data_pred.shape)\n",
        "    print(\"\")\n",
        "    loss += tf.reduce_mean(tf.square(data_train - data_pred))\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "BucHRJ9OvAPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_grad(model, X_train, data_train):\n",
        "\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # This tape is for derivatives with\n",
        "        # respect to trainable variables\n",
        "        tape.watch(model.trainable_variables)\n",
        "        loss = compute_loss(model, X_train, data_train)\n",
        "\n",
        "    g = tape.gradient(loss, model.trainable_variables)\n",
        "    del tape\n",
        "\n",
        "    return loss, g"
      ],
      "metadata": {
        "id": "h-H_nyrUkELj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model aka u_\\theta\n",
        "model = init_model()\n",
        "\n",
        "# We choose a piecewise decay of the learning rate, i.e., the\n",
        "# step size in the gradient descent type algorithm\n",
        "# the first 1000 steps use a learning rate of 0.01\n",
        "# from 1000 - 3000: learning rate = 0.001\n",
        "# from 3000 onwards: learning rate = 0.0005\n",
        "\n",
        "lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay([1000,3000],[1e-2,1e-3,5e-4])\n",
        "\n",
        "# Choose the optimizer\n",
        "optim = tf.keras.optimizers.Adam(learning_rate=lr)"
      ],
      "metadata": {
        "id": "uNX1Rw8qlgDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "\n",
        "# Define one training step as a TensorFlow function to increase speed of training\n",
        "@tf.function\n",
        "def train_step():\n",
        "    # Compute current loss and gradient w.r.t. parameters\n",
        "    loss, grad_theta = get_grad(model, X_train, data_train)\n",
        "\n",
        "    # Perform gradient descent step\n",
        "    optim.apply_gradients(zip(grad_theta, model.trainable_variables))\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Number of training epochs\n",
        "N = 500\n",
        "hist = []\n",
        "\n",
        "# Start timer\n",
        "t0 = time()\n",
        "\n",
        "for i in range(N+1):\n",
        "\n",
        "    loss = train_step()\n",
        "\n",
        "    # Append current loss to hist\n",
        "    hist.append(loss.numpy())\n",
        "\n",
        "    # Output current loss after 50 iterates\n",
        "    if i%50 == 0:\n",
        "        print('It {:05d}: loss = {:10.8e}'.format(i,loss))\n",
        "\n",
        "# Print computation time\n",
        "print('\\nComputation time: {} seconds'.format(time()-t0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot4zR65GlqGt",
        "outputId": "7491a1ed-19e6-4158-99e0-5994061f69e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max and min of u <bound method amax of <tf.Tensor 'gradient_tape/strided_slice_4/StridedSliceGrad:0' shape=(5000, 1) dtype=float64>> <bound method amin of <tf.Tensor 'gradient_tape/strided_slice_4/StridedSliceGrad:0' shape=(5000, 1) dtype=float64>>\n",
            "type of v <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
            "max and min of u_x <bound method amax of <tf.Tensor 'gradient_tape/strided_slice_3/StridedSliceGrad_2:0' shape=(5000, 1) dtype=float64>> <bound method amin of <tf.Tensor 'gradient_tape/strided_slice_3/StridedSliceGrad_2:0' shape=(5000, 1) dtype=float64>>\n",
            "max and min of u_y <bound method amax of <tf.Tensor 'gradient_tape/strided_slice_4/StridedSliceGrad_3:0' shape=(5000, 1) dtype=float64>> <bound method amin of <tf.Tensor 'gradient_tape/strided_slice_4/StridedSliceGrad_3:0' shape=(5000, 1) dtype=float64>>\n",
            "max and min v_x <bound method amax of <tf.Tensor 'gradient_tape/strided_slice_3/StridedSliceGrad_4:0' shape=(5000, 1) dtype=float64>> <bound method amin of <tf.Tensor 'gradient_tape/strided_slice_3/StridedSliceGrad_4:0' shape=(5000, 1) dtype=float64>>\n",
            "tf.concat([f,g], axis = 1).shape (5000, 2)\n",
            "\n",
            "################Added by Yoshi##################\n",
            "This is data_pred.shape\n",
            "(5000, 3)\n",
            "\n",
            "max and min of u <bound method amax of <tf.Tensor 'gradient_tape/strided_slice_4/StridedSliceGrad:0' shape=(5000, 1) dtype=float64>> <bound method amin of <tf.Tensor 'gradient_tape/strided_slice_4/StridedSliceGrad:0' shape=(5000, 1) dtype=float64>>\n",
            "type of v <class 'tensorflow.python.framework.ops.SymbolicTensor'>\n",
            "max and min of u_x <bound method amax of <tf.Tensor 'gradient_tape/strided_slice_3/StridedSliceGrad_2:0' shape=(5000, 1) dtype=float64>> <bound method amin of <tf.Tensor 'gradient_tape/strided_slice_3/StridedSliceGrad_2:0' shape=(5000, 1) dtype=float64>>\n",
            "max and min of u_y <bound method amax of <tf.Tensor 'gradient_tape/strided_slice_4/StridedSliceGrad_3:0' shape=(5000, 1) dtype=float64>> <bound method amin of <tf.Tensor 'gradient_tape/strided_slice_4/StridedSliceGrad_3:0' shape=(5000, 1) dtype=float64>>\n",
            "max and min v_x <bound method amax of <tf.Tensor 'gradient_tape/strided_slice_3/StridedSliceGrad_4:0' shape=(5000, 1) dtype=float64>> <bound method amin of <tf.Tensor 'gradient_tape/strided_slice_3/StridedSliceGrad_4:0' shape=(5000, 1) dtype=float64>>\n",
            "tf.concat([f,g], axis = 1).shape (5000, 2)\n",
            "\n",
            "################Added by Yoshi##################\n",
            "This is data_pred.shape\n",
            "(5000, 3)\n",
            "\n",
            "It 00000: loss = 3.18596622e-01\n",
            "It 00050: loss = 3.88734318e-02\n",
            "It 00100: loss = 3.03046543e-02\n",
            "It 00150: loss = 2.63610781e-02\n",
            "It 00200: loss = 2.60323841e-02\n",
            "It 00250: loss = 2.30550109e-02\n",
            "It 00300: loss = 2.18781595e-02\n",
            "It 00350: loss = 2.95070448e-02\n",
            "It 00400: loss = 1.97151854e-02\n",
            "It 00450: loss = 1.89824085e-02\n",
            "It 00500: loss = 1.89333499e-02\n",
            "\n",
            "Computation time: 407.1505787372589 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"navier-stokes-pinn.keras\")"
      ],
      "metadata": {
        "id": "q4wZ_37TL3o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_pred = model.predict(X_test)\n",
        "# convert data_pred to tensor for gradientTape\n",
        "data_tf = tf.convert_to_tensor(data_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aelDDCHs686c",
        "outputId": "0f5c19ca-2a20-4176-f0d3-5ab4f8937fab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "157/157 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.GradientTape(persistent=True) as tape:\n",
        "\n",
        "  x,y,t = X_test[:,0:1], X_test[:,1:2], X_test[:,2:3]\n",
        "  tape.watch(x)\n",
        "  tape.watch(y)\n",
        "  data_pred = model(tf.stack([x[:,0],y[:,0],t[:,0]], axis=1))\n",
        "  psi_pred = data_pred[:,0:1]\n",
        "  p_pred = data_pred[:,1:2]\n",
        "u = tape.gradient(psi_pred, y)\n",
        "print(type(u))\n",
        "v = -1 * tape.gradient(psi_pred, x)\n",
        "data_pred = tf.concat([u,v,p_pred], axis = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O93jtdk4GkWn",
        "outputId": "d57936e1-f4db-4ce7-9b96-154fb6c518d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # understand how model and model.predict work\n",
        "# p_model_predict = data_tf[:,1:2]\n",
        "# error1 = tf.reduce_mean(tf.square(p_pred - p_model_predict))\n",
        "# print(p_model_predict)\n",
        "# print(p_pred)\n",
        "# print(error1)"
      ],
      "metadata": {
        "id": "07C75VPhOzmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error = tf.reduce_mean(tf.square(data_test - data_pred))\n",
        "print(error)"
      ],
      "metadata": {
        "id": "BbN7dOBDMBmZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f371746-8363-4dad-fca2-b3c3858b505f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(0.01498067142718852, shape=(), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization keeps crashing; should do this on HYAK"
      ],
      "metadata": {
        "id": "GL6cbDce0a0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from mpl_toolkits.mplot3d import Axes3D\n",
        "# from matplotlib import cm\n",
        "# from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
        "# import matplotlib.pyplot as plt\n",
        "# # visualize predicted data and testing data at snap.\n",
        "# x,y,t = X_test[:,0:1], X_test[:,1:2], X_test[:,2:3]\n",
        "# X,Y = np.meshgrid(x, y) # grid of point\n",
        "# Z = data_pred[:, 0:1] # evaluation of the function on the grid\n",
        "\n",
        "# fig = plt.figure()\n",
        "# ax = fig.add_subplot(projection = '3d')\n",
        "# surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
        "#                       cmap=cm.RdBu,linewidth=0, antialiased=False)\n",
        "\n",
        "# ax.zaxis.set_major_locator(LinearLocator(10))\n",
        "# ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
        "\n",
        "# fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "1t5BdPt4sk9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model('navier-stokes-pinn.keras')\n",
        "\n",
        "# Show the model architecture\n",
        "new_model.summary()"
      ],
      "metadata": {
        "id": "F2iV_fQ2O671",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2057dea9-144b-4689-f684-624c1030a795"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 20)                80        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 20)                420       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 2)                 42        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3062 (23.92 KB)\n",
            "Trainable params: 3062 (23.92 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}